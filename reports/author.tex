%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Construction of a proper prior for a Bayesian Envelope Model}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\subtitle{\emph{Costruzione di una prior propria per un modello Envelope bayesiano}}
\author{Andrea Mascaretti}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Andrea Mascaretti \at University of Padova, Via Cesare Battisti, 241, 35121, Padova (PD), Italy, \email{mascaretti@stat.unipd.it}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\abstract{Envelope models are multivariate linear regression
  techniques that aims at reducing the variance of the
  estimator. Bayesian envelopes allow to quantify the uncertainty of
  inference by means of the posterior distribution. In this work, we
  construct a proper prior distribution and compare it to the existing
  literature. A prior sensitivity analysis is conducted, yielding similar
  results.}  \abstract{\emph{I modelli envelope sono una particolare tipologia di regressione lineare multivariata finalizzata a ridurre la varianze degli stimatori. La formulazione bayesiana di questi modelli consente di quantificare direttamente l'incertezza degli stimatori mediante l'analisi della posterior. In questo lavoro, proponiamo una prior propria per il modello e valutiamo l'impatto della prior sull'inferenza, ottenendo risultati comparabili alle proposte presenti in letteratura.}} \keywords{envelope models, bayesian statistics}

\section{Response Envelopes}
\label{sec:1}
Envelopes
\cite{cookENVELOPEMODELSPARSIMONIOUS2010,cookIntroductionEnvelopesDimension2018}
are a class of models aimed at increasing the efficiency of
multivariate regression by exploiting the relations between response
and predictors that affect the accuracy of the results and are not
taken into account by standard methods.  Within the usual multivariate
regression setting, the expected value of a random variable
$Y \in R^r$ is given a functional form such that we get
\begin{equation}
  \label{eq:1}
  Y_i = \mu + \beta X_i + \varepsilon_i, \;\; i = 1, \dots, n,
\end{equation}

where $\left\{X_i\right\}_{i = 1}^n$ is a sequence of non-stochastic
vectors, with $X_i \in R^p$ for $i = 1, \dots, n$, the errors are
independent and identically distributed multivariate normal vectors
with zero mean and covariance $\Sigma$, $\mu \in R^r$ is an unknown
vector of intercepts and $\beta \in M_{\left( r, p \right)}$ (where
$M_{\left( a, b \right)}$ denotes the space of real matrices of dimensions
$\left( a, b \right)$) is the unknown vector of regression
coefficients.  For simplicity (and without loss of generality), we
assume that the predictors are centred, $\sum_{i = 1}^n X_i =
0$. Moreover, let $Y$ be the $\left( n \times r \right)$ matrix of
rows $\left(Y_i - \bar{Y}\right)^T$, where $\bar{Y}$ is the sample
mean, and $Y_0$ be the non-centred matrix. In a similar fashion, let
$X = \left\{X_i^T\right\}$ be the matrix of the predictors,
$S_{Y, X} = n^{-1} Y^TX$ and $S_X = n^{-1}X^TX$. The maximum
likelihood estimator,

\begin{equation}
  \label{eq:2}
  \hat{\beta} = S_{Y, X}S_X^{-1},
\end{equation}

is incidentally equal the ordinary least squares estimator. From Eq.
\ref{eq:2}, we notice that this is akin to performing $r$ separate
univariate regressions: one for every element of $Y$ on $X$. Inference
on $\beta_{j,k}$, the $\left( j, k \right)$th element of $\beta$ is
the same we would obtain by constructing a univariate model. The model
in Eq. \ref{eq:1} becomes operational when inference is conducted
simultaneously on different rows of $\beta$ or various elements of $Y$
jointly.

The intuition behind envelope models is that there might be linear
combinations of the response vectors whose distribution is invariant
with respect to the non-stochastic predictors. Explicitly modelling
for this property allows to obtain estimator whose variance is
reduced. We call such linear combinations of $Y$ $X$-invariant. Notice
that for a linear transformation $G \in M_{\left( r, q \right)}$, with
$q \leq r$, if $G^T Y$ is invariant, then also $A^T G^T Y$ has the
same property for any non-stochastic matrix
$A \in M_{\left( q, q \right)}$. In other words, only
$\mathrm{span}\left( G \right)$ is identifiable.

From a mathematical point of view, this is equivalent to assuming the
existence of two matrices $\Gamma$ and $\Gamma_0$ such that
$O = \left[ \Gamma \;  \Gamma_0 \right]$ is orthogonal. We obtain

\begin{enumerate}
\item $\Gamma_0^T Y | X \sim \Gamma_0^T Y$
\item $\Gamma^T Y \perp \Gamma_0^T Y | X$
\end{enumerate}

The conditions above entail that
$\mathrm{span}\left( \beta \right) \subseteq \mathrm{span}\left(
  \Gamma \right)$ and
$\Sigma = \Sigma_1 + \Sigma_2 = P_{\Gamma}\Sigma P_{\Gamma} +
Q_{\Gamma} \Sigma Q_{\Gamma}$, where $P_{\left(\cdot\right)}$ is the
orthogonal projector operation on a space and
$Q_{\left(\cdot\right)} = I - P_{\left(\cdot\right)}$ is the
projection on the orthogonal space. In this scenario,
$\mathrm{span}\left( \Gamma \right)$ is a reducing subspace of
$\Sigma$ (\cite{cookENVELOPEMODELSPARSIMONIOUS2010}). The $\Sigma$-envelope of $\mathcal{B} = \mathrm{span}\left( \beta \right)$, $\mathcal{E}_{\Sigma} \left( \mathcal{B} \right)$, is the smallest reducing subspace of $\Sigma$ that contains $\mathcal{B}$.

Model in Eq. \ref{eq:1} can be rewritten as

\begin{equation}
  \label{eq:3}
  Y_i = \mu + \Gamma \eta X_i + \varepsilon,
\end{equation}
where $\beta = \Gamma \eta$, $\Gamma \in M_{\left( r, u \right)}$ is
an orthogonal basis of
$\mathcal{E}_{\Sigma}\left( \mathcal{B} \right)$ and $u$ is the
dimension of the envelope
$\mathcal{E}_{\Sigma}\left( \mathcal{B} \right)$. Moreover, the
variance is
$\Sigma = \Sigma_1 + \Sigma_2 = \Gamma \Omega \Gamma^T + \Gamma_0
\Omega_0 \Gamma_0^T$, where $\Omega \in M_{\left( u, u \right)}$ and $\Omega_0 \in M_{\left( r-u, r-u \right)}$ are two diagonal matrices carrying the coordinate information with respect to the basis $\Gamma$ and $\Gamma_0$.

\subsection{Bayesian Envelopes}
The only contribution, to the best of our knowledge, on Bayesian
envelopes models is \cite{khareBayesianApproachEnvelope2017}. The
rationale behind Bayesian envelopes is that it allows to quantify the
uncertainty of the predictions by computing the posterior distribution
(as opposed to bootstrap or asymptotic considerations), as well as
extending the model to the cases where $n < r$. Moreover, prior
information can be incorporated into the learning process, be it on
the values of the parameters or to induce sparsity or other desirable
properties. As for the selection of $u$, the dimension of the
envelope, \cite{khareBayesianApproachEnvelope2017} adopt a Deviance
Information Criterion to obtain the best value, in lieu of the
Likelihood Ratio Tests used within the frequentist framework.  The
interest in obtaining a proper prior distribution for a Bayesian
envelope stems from the fact that this is a prerequisite to extend it
to more complex scenarios, such as mixtures or nonparametric
formulations.

The prior distribution is defined on the parameters
$\left( \mu, \eta, \left(\Gamma, \Gamma_0\right), \Omega, \Omega_0
\right)$. Notice that, for identifiability, we constrain $\Omega$ and
$\Omega_0$ to be diagonal matrices with entries disposed in decreasing
order. This is equivalent to post-multiplying $\Gamma$ and $\Gamma_0$
by the matrices of eigenvectors of the original $\Omega$ and
$\Omega_0$. From a mathematical point of view, this is equivalent to
fix $\Gamma$ and $\Gamma_0$ to be bases of the envelope and, thus, as
elements of a subset of a Stiefel manifold restricted to have that the
maximum element for each column as positive sign, denoted by
$S^+_{\left( \cdot,\; \cdot \right)}$. In this respect, we notice that
the Stiefel manifold of arbitrary finite dimensions
$\left( a, a \right)$ is a compact unimodular group with a unique Haar
measure, which induces a measure on $S_{\left( a, b\right)}$ and
$S^+_{\left( a, b \right)}$.

The parameter space is then given by
$M_{\left( r, 1 \right)} \times M_{\left( u, p \right)}\times
S^+_{\left( r, r \right)} \times O_u \times O_{r -u} $, where $O_a$ is
the set of diagonal matrices of dimension $a$ with entries disposed in
decreasing order.

We define the prior on the parameters as follows:

\begin{enumerate}
\item $\mu$ is set to be independent from the other parameters. We endow it with a multivariate normal distribution, so that $\pi \left( \mu \right) = \mathcal{N}_r \left( \mu_0, \Sigma_0 \right)$,
\item The conditional prior on $\eta$ is a matrix normal: $$\pi \left( \eta | \left( \Gamma, \Gamma_0, \Omega, \Omega_0 \right) \right) = \mathcal{N}_{\left( u, p \right)}\left( \Gamma^T, \Omega, C^{-1} \right),$$ where $C^{-1}$ is a positive definite matrix in $M_{\left( p, p \right)}$.
\item The prior on $O = \left( \Gamma, \Gamma_0 \right)$ is a matrix Bingham distribution with parameters $G$ and $D$, where $G$ is a positive semi-definite matrix in $M_{\left( r, r \right)}$ and $D$ is in $O_r$ with positive entries. Thus, $\pi \left( O \right) = \mathcal{B}_{\left( r, r \right)}\left( G, D^{-1} \right)$. The density is proportional to $\exp{\left\{ \left(-1/2\right) \mathrm{tr}\left( D^{-1}O^TGO\right) \right\}}$
\item Denoting by $\omega$ and $\omega_0$ the diagonal vectors of, respectively, $\Omega$ and $\Omega_0$, we assume that, a priori, they are distributed as order statistics of $u$ and $r - u$ independent and identically distributed observations from Inverse-Gamma distributions of shape and rate parameters $\alpha$, $\psi$ and $\alpha_0$, $\psi_0$.
\end{enumerate}

Notice that the main difference between our work and
\cite{khareBayesianApproachEnvelope2017} is the prior on $\mu$. From a
computational point of view, this means that the structure of the
Gibbs sampler is similar, the only difference being the structure of
the full-conditional for $\mu$, which can be easily computed to be of
the form

\begin{equation}
  \label{eq:6}
  \pi \left( \mu | \eta, \left( \Gamma, \Gamma_0 \right), \omega, \omega_0, Y \right) = \mathcal{N}_r\left( \mu_{c}, \Sigma_{c} \right),
\end{equation}

where $$\Sigma_{c} = \left( \Sigma^{-1}_0 + \left(\frac{\Sigma}{n} \right)^{-1}\right)^{-1},$$ and $$\mu_{c} = \Sigma_{c} \left( \Sigma_0^{-1} \mu_0 + \left(\frac{\Sigma}{n} \right)^{-1} \bar{Y} \right).$$

Notice that the Harris ergodicity of the chain is also a
straightforward extension of \cite{khareBayesianApproachEnvelope2017}.

\section{Simulation and Data Analysis}
We now perform a test for different values of the prior distribution
on a synthetic dataset. The aim is to assess the sensitivity with
respect to the choice of the hyperparameters. We generated $n = 100$
data points from a normal distribution with zero mean and identity
matrix as covariance. We set $u = 1$, $p = 2$, $r = 3$. The parameters
are defined as follows:

\begin{enumerate}
\item $\mu = \left( 12, 12, 12 \right)$
\item $\omega = 6.2$
\item $\omega_0 = \left( 3.2, 1.4 \right)$
\item $O = I_r$
\end{enumerate}

and $Y_i$ are randomly drawn a multivariate normal with mean
$\mu + \Gamma \eta X_i$ and covariance
$\Gamma \Omega \Gamma^T + \Gamma_0 \Omega_0 \Gamma_{0}^T$.  As for the
hyperparamaters, we distinguish between three cases. We focus on $\mu$
as it is the most relevant change we make. In the first case, we use a
weakly informative proper prior with $\mu_0 = \left( 0, 0, 0 \right)$
and $\Sigma_0 = \kappa I_r$, with $\kappa = 10$. In the second test
case, we set $\mu_0 = \bar{Y}$ and $\Sigma = I_{r}$. Finally,
we consider the improper prior as in
\cite{khareBayesianApproachEnvelope2017}. The other parameters are set
as follows: $C = I_p$, $D = I_r$, $G = I_r$, $\alpha = 3$, $\psi = 3$, $\alpha_0 = 3$ and $\psi_0 = 3$.

For each case study, we run a Gibbs sampler for 1000 iterations, with
a burn in of 300. The initialisation for each chain was from the same
random point in the parameter space.

Results for the three components of $\mu$ are reported, respectively
in Tables \ref{tab:1}, \ref{tab:2}, and \ref{tab:3}.

We see that even though the empirical and the noninformative priors
lead to somewhat closer posterior estimates, the effect of placing a
weakly informative prior also yields posterior higher density
intervals that are in line with the other two classes of prior
distributions. However, the true advantage of a proper prior is that
it allows for extending the model to more complex settings. The fact
that it yields similar results notwithstanding different
hyperparameters is certainly encouraging, although, as always, some
care should be put in their refinement.

\begin{table}
\caption{Posterior inference for $\mu = \left( \mu_{1}, \mu_{2}, \mu_{3} \right)$ with a weakly informative prior: posterior higher density interval (HDI) are reported.}
\label{tab:1}       % Give a unique label
%
% Follow this input for your own table layout
%
\begin{tabular}{p{2cm}p{2.4cm}p{2cm}p{4.9cm}}
\hline\noalign{\smallskip}
Parameter & Mean & 3\% HDI & 97\% HDI \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
$\mu_{1}$ & 12.67  & 12.005 & 13.359 \\
$\mu_2$ & 12.043 & 11.715 & 12.324\\
$\mu_3$ & 12.097  & 11.882 & 12.313 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

\begin{table}
\caption{Posterior inference for $\mu = \left( \mu_{1}, \mu_{2}, \mu_{3}\right)$ with an empirical prior: posterior higher density interval (HDI) are reported.}
\label{tab:2}       % Give a unique label
%
% Follow this input for your own table layout
%
\begin{tabular}{p{2cm}p{2.4cm}p{2cm}p{4.9cm}}
\hline\noalign{\smallskip}
Parameter & Mean & 3\% HDI & 97\% HDI \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
$\mu_{1}$ & 12.832  & 12.1 & 13.49 \\
$\mu_2$ & 12.071 & 11.746 & 12.361\\
$\mu_3$ & 12.117  & 11.903 & 12.307 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}


\begin{table}
\caption{Posterior inference for $\mu = \left( \mu_{1}, \mu_{2}, \mu_{3}\right)$ with a noninformative prior: posterior higher density interval (HDI) are reported.}
\label{tab:3}       % Give a unique label
%
% Follow this input for your own table layout
%
\begin{tabular}{p{2cm}p{2.4cm}p{2cm}p{4.9cm}}
\hline\noalign{\smallskip}
Parameter & Mean & 3\% HDI & 97\% HDI \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
$\mu_{1}$ & 12.85  & 12.246 & 13.425 \\
$\mu_2$ & 12.07 & 11.74 & 12.364\\
$\mu_3$ & 12.121  & 11.916 & 12.286 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

\section{Conclusions}
In this work, we have constructed a proper prior distribution for a
Bayesian envelope model. We carried out an assessment of the prior
sensitivity on a simple test case, obtaining that the choice of the
hyperparameters for the parameter $\mu$ yield similar results in the
three cases studied: a weakly informative, an empirical one and a
non-informative prior. As such, this model endowed with a proper prior
can be extended to more complex scenarios. For instance, it can be
used as a building block for a mixture model, as opposed to the a
model with an improper prior. This is especially true given the
stability of posterior HDIs in three cases.

\input{referenc}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-parse-self: t
%%% TeX-auto-save: t
%%% End:
