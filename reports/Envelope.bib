
@book{albertBayesianComputation2009,
  title = {Bayesian Computation with {{R}}},
  author = {Albert, Jim},
  year = {2009},
  series = {Use {{R}}!},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-92297-3},
  lccn = {QA279.5 .A53 2009},
  keywords = {Bayesian statistical decision theory,Data processing,R (Computer program language)},
  annotation = {OCLC: ocn297148506},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Albert_2009_Bayesian_computation_with_R.pdf}
}

@book{amariDifferentialGeometryStatistical1987,
  title = {Differential Geometry in Statistical Inference},
  editor = {Amari, Shun'ichi},
  year = {1987},
  series = {Lecture Notes-Monograph Series},
  number = {v. 10},
  publisher = {{Institute of Mathematical Statistics}},
  address = {{Hayward, Calif}},
  isbn = {978-0-940600-12-6},
  lccn = {QA276 .D53 1987},
  keywords = {Geometry; Differential,Mathematical statistics},
  file = {/home/andrea/gdrive/phd/zotero_files/Amari_1987_Differential_geometry_in_statistical_inference.pdf}
}

@book{andersonIntroductionMultivariateStatistical2003,
  title = {An Introduction to Multivariate Statistical Analysis},
  author = {Anderson, T. W.},
  year = {2003},
  series = {Wiley Series in Probability and Statistics},
  edition = {3rd ed},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-471-36091-9},
  lccn = {QA278 .A516 2003},
  keywords = {Multivariate analysis},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Anderson_2003_An_introduction_to_multivariate_statistical_analysis.pdf}
}

@article{arashiBayesianAnalysisMultivariate2014,
  title = {Bayesian Analysis in Multivariate Regression Models with Conjugate Priors},
  author = {Arashi, M. and Iranmanesh, Anis and Norouzirad, M. and Salarzadeh Jenatabadi, Hashem},
  year = {2014},
  month = nov,
  journal = {Statistics},
  volume = {48},
  number = {6},
  pages = {1324--1334},
  publisher = {{Taylor \& Francis}},
  issn = {0233-1888},
  doi = {10.1080/02331888.2013.809720},
  url = {https://doi.org/10.1080/02331888.2013.809720},
  urldate = {2021-12-11},
  abstract = {In this paper, we consider the full rank multivariate regression model with matrix elliptically contoured distributed errors. We formulate a conjugate prior distribution for matrix elliptical models and derive the posterior distributions of mean and scale matrices. In the sequel, some characteristics of regression matrix parameters are also proposed.},
  keywords = {conjugate prior,generalized Wishart distribution,inverse Laplace transform,matrix elliptical distribution,Primary: 62F15,Secondary: 62H05},
  annotation = {\_eprint: https://doi.org/10.1080/02331888.2013.809720},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Arashi_et_al_2014_Bayesian_analysis_in_multivariate_regression_models_with_conjugate_priors.pdf;/home/andrea/Zotero/storage/5NRESVKY/02331888.2013.html}
}

@article{BayesianMultivariateLinear2021,
  title = {Bayesian Multivariate Linear Regression},
  year = {2021},
  month = oct,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Bayesian_multivariate_linear_regression&oldid=1051561052},
  urldate = {2022-01-13},
  abstract = {In statistics, Bayesian multivariate linear regression is a Bayesian approach to multivariate linear regression, i.e. linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1051561052},
  file = {/home/andrea/Zotero/storage/CI4NY35B/index.html}
}

@book{bijmaIntroductionMathematicalStatistics2017,
  title = {{An Introduction to Mathematical Statistics}},
  author = {Bijma, Fetsje and Jonker, Marianne and Vaart, Aad Van Der},
  year = {2017},
  edition = {1\textdegree{} edizione},
  publisher = {{Amsterdam Univ Pr}},
  address = {{Amsterdam}},
  isbn = {978-94-6298-510-0},
  langid = {Inglese},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Bijma_et_al_2017_An_Introduction_to_Mathematical_Statistics.pdf}
}

@book{boxBayesianInferenceStatistical1992,
  title = {Bayesian Inference in Statistical Analysis},
  author = {Box, George E. P. and Tiao, George C.},
  year = {1992},
  series = {Wiley Classics Library},
  publisher = {{J. Wiley and sons}},
  address = {{New York Chichester Brisbane [etc]}},
  isbn = {978-0-471-57428-6},
  langid = {english},
  lccn = {519.54},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Box_Tiao_1992_Bayesian_inference_in_statistical_analysis.pdf}
}

@book{brooksHandbookMarkovChain2011,
  title = {Handbook for {{Markov}} Chain {{Monte Carlo}}},
  editor = {Brooks, Steve},
  year = {2011},
  publisher = {{Taylor \& Francis}},
  address = {{Boca Raton}},
  isbn = {978-1-4200-7941-8},
  lccn = {QA274.7 .H346 2011},
  keywords = {Markov processes,Monte Carlo method}
}

@article{cookDimensionReductionRegression2007,
  title = {Dimension Reduction in Regression without Matrix Inversion},
  author = {Cook, R. Dennis and Li, Bing and Chiaromonte, Francesca},
  year = {2007},
  month = aug,
  journal = {Biometrika},
  volume = {94},
  number = {3},
  pages = {569--584},
  issn = {0006-3444},
  doi = {10.1093/biomet/asm038},
  url = {https://doi.org/10.1093/biomet/asm038},
  urldate = {2021-10-07},
  abstract = {Regressions in which the fixed number of predictors p exceeds the number of independent observational units n occur in a variety of scientific fields. Sufficient dimension reduction provides a promising approach to such problems, by restricting attention to d~\&lt;~n linear combinations of the original p predictors. However, standard methods of sufficient dimension reduction require inversion of the sample predictor covariance matrix. We propose a method for estimating the central subspace that eliminates the need for such inversion and is applicable regardless of the (n, p) relationship. Simulations show that our method compares favourably with standard large sample techniques when the latter are applicable. We illustrate our method with a genomics application.},
  keywords = {envelope},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Cook_et_al_2007_Dimension_reduction_in_regression_without_matrix_inversion.pdf}
}

@article{cookENVELOPEMODELSPARSIMONIOUS2010,
  title = {{{ENVELOPE MODELS FOR PARSIMONIOUS AND EFFICIENT MULTIVARIATE LINEAR REGRESSION}}},
  author = {Cook, R. Dennis and Li, Bing and Chiaromonte, Francesca},
  year = {2010},
  journal = {Statistica Sinica},
  volume = {20},
  number = {3},
  pages = {927--960},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  url = {https://www.jstor.org/stable/24309466},
  urldate = {2021-07-27},
  abstract = {We propose a new parsimonious version of the classical multivariate normal linear model, yielding a maximum likelihood estimator (MLE) that is asymptotically less variable than the MLE based on the usual model. Our approach is based on the construction of a link between the mean function and the covariance matrix, using the minimal reducing subspace of the latter that accommodates the former. This leads to a multivariate regression model that we call the envelope model, where the number of parameters is maximally reduced. The MLE from the envelope model can be substantially less variable than the usual MLE, especially when the mean function varies in directions that are orthogonal to the directions of maximum variation for the covariance matrix.},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Cook_et_al_2010_ENVELOPE_MODELS_FOR_PARSIMONIOUS_AND_EFFICIENT_MULTIVARIATE_LINEAR_REGRESSION.pdf}
}

@book{cookIntroductionEnvelopesDimension2018,
  title = {An Introduction to Envelopes: Dimension Reduction for Efficient Estimation in Multivariate Statistics},
  shorttitle = {An Introduction to Envelopes},
  author = {Cook, R. Dennis},
  year = {2018},
  series = {Wiley Series in Probability and Statistics},
  edition = {1st edition},
  publisher = {{John Wiley \& Sons}},
  address = {{Hoboken, NJ}},
  isbn = {978-1-119-42293-8},
  lccn = {QA278 .C648 2018},
  keywords = {Dimension reduction (Statistics),Multivariate analysis},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Cook_2018_An_introduction_to_envelopes.pdf}
}

@article{cookNoteFastEnvelope2016,
  ids = {cookNoteFastEnvelope2016a},
  title = {A Note on Fast Envelope Estimation},
  author = {Cook, R. Dennis and Forzani, Liliana and Su, Zhihua},
  year = {2016},
  month = sep,
  journal = {Journal of Multivariate Analysis},
  volume = {150},
  pages = {42--54},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2016.05.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X16300276},
  urldate = {2021-09-29},
  abstract = {We propose a new algorithm for envelope estimation, along with a new n-consistent method for computing starting values. The new algorithm, which does not require optimization over a Grassmannian, is shown by simulation to be much faster and typically more accurate than the best existing algorithm proposed by Cook and Zhang (2016).},
  langid = {english},
  keywords = {Envelopes,Grassmann manifold,Reducing subspaces},
  file = {/home/andrea/Zotero/storage/T7DBP36X/S0047259X16300276.html}
}

@article{cookSliceMultivariateDimension2022,
  ids = {cookSliceMultivariateDimension2022a},
  title = {A Slice of Multivariate Dimension Reduction},
  author = {Cook, R. Dennis},
  year = {2022},
  month = mar,
  journal = {Journal of Multivariate Analysis},
  series = {50th {{Anniversary Jubilee Edition}}},
  volume = {188},
  pages = {104812},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2021.104812},
  url = {https://www.sciencedirect.com/science/article/pii/S0047259X21000907},
  urldate = {2021-12-30},
  abstract = {We describe how many dimension reduction strategies are connected conceptually and philosophically, paving the way for a unified approach to multivariate dimension reduction in statistics. Specific methods covered include envelopes, sufficient dimension reduction methods like SIR and SAVE, principal components, principal fitted components, and partial least squares.},
  langid = {english},
  keywords = {envelope,Envelopes,Partial least squares,Principal components,Principal fitted components,Sufficient dimension reduction},
  file = {/home/andrea/gdrive/phd/zotero_files/Cook_2022_A_slice_of_multivariate_dimension_reduction.pdf}
}

@article{dawidMatrixvariateDistributionTheory1981,
  title = {Some Matrix-Variate Distribution Theory: {{Notational}} Considerations and a {{Bayesian}} Application},
  shorttitle = {Some Matrix-Variate Distribution Theory},
  author = {Dawid, A. P.},
  year = {1981},
  journal = {Biometrika},
  volume = {68},
  number = {1},
  pages = {265--274},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/68.1.265},
  url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/68.1.265},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Dawid_1981_Some_matrix-variate_distribution_theory.pdf}
}

@article{duanBayesianConstraintRelaxation2020,
  title = {Bayesian Constraint Relaxation},
  author = {Duan, Leo L and Young, Alexander L and Nishimura, Akihiko and Dunson, David B},
  year = {2020},
  month = mar,
  journal = {Biometrika},
  volume = {107},
  number = {1},
  pages = {191--204},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz069},
  url = {https://doi.org/10.1093/biomet/asz069},
  urldate = {2022-01-05},
  abstract = {Prior information often takes the form of parameter constraints. Bayesian methods include such information through prior distributions having constrained support. By using posterior sampling algorithms, one can quantify uncertainty without relying on asymptotic approximations. However, sharply constrained priors are not necessary in some settings and tend to limit modelling scope to a narrow set of distributions that are tractable computationally. We propose to replace the sharp indicator function of the constraint with an exponential kernel, thereby creating a close-to-constrained neighbourhood within the Euclidean space in which the constrained subspace is embedded. This kernel decays with distance from the constrained space at a rate depending on a relaxation hyperparameter. By avoiding the sharp constraint, we enable use of off-the-shelf posterior sampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic computation in a broad range of models. We study the constrained and relaxed distributions under multiple settings and theoretically quantify their differences. Application of the method is illustrated through several novel modelling examples.},
  keywords = {envelope},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Duan_et_al_2020_Bayesian_constraint_relaxation.pdf;/home/andrea/Zotero/storage/JVZ77QQH/5686747.html}
}

@article{dutilleulMleAlgorithmMatrix1999,
  title = {The Mle Algorithm for the Matrix Normal Distribution},
  author = {Dutilleul, Pierre},
  year = {1999},
  month = sep,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {64},
  number = {2},
  pages = {105--123},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949659908811970},
  url = {https://www.tandfonline.com/doi/full/10.1080/00949659908811970},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Dutilleul_1999_The_mle_algorithm_for_the_matrix_normal_distribution.pdf}
}

@book{fuhrerScientificComputingPython2021,
  title = {Scientific Computing with {{Python}}: High-Performance Scientific Computing with {{NumPy}}, {{SciPy}}, and Pandas},
  shorttitle = {Scientific Computing with {{Python}}},
  author = {F{\"u}hrer, Claus and Verdier, Olivier and Solem, Jan Erik},
  year = {2021},
  edition = {Second edition},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  isbn = {978-1-83882-232-3},
  langid = {english},
  file = {/home/andrea/Zotero/storage/JZMMN3CV/Führer et al. - 2021 - Scientific computing with Python high-performance.pdf}
}

@book{gamermanMarkovChainMonte2006,
  title = {Markov Chain {{Monte Carlo}}: Stochastic Simulation for {{Bayesian}} Inference},
  shorttitle = {Markov Chain {{Monte Carlo}}},
  author = {Gamerman, Dani and Lopes, Hedibert Freitas},
  year = {2006},
  series = {Texts in Statistical Science Series},
  edition = {2nd ed},
  number = {68},
  publisher = {{Taylor \& Francis}},
  address = {{Boca Raton}},
  isbn = {978-1-58488-587-0},
  lccn = {QA279.5 .G36 2006},
  keywords = {Bayesian statistical decision theory,Markov processes,Monte Carlo method},
  file = {/home/andrea/gdrive/phd/zotero_files/Gamerman_Lopes_2006_Markov_chain_Monte_Carlo.pdf}
}

@article{garthwaiteInterpretationPartialLeast1994,
  title = {An {{Interpretation}} of {{Partial Least Squares}}},
  author = {Garthwaite, Paul H.},
  year = {1994},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  pages = {122--127},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1994.10476452},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476452},
  urldate = {2021-12-12},
  abstract = {Univariate partial least squares (PLS) is a method of modeling relationships between a Y variable and other explanatory variables. It may be used with any number of explanatory variables, even far more than the number of observations. A simple interpretation is given that shows the method to be a straightforward and reasonable way of forming prediction equations. Its relationship to multivariate PLS, in which there are two or more Y variables, is examined, and an example is given in which it is compared by simulation with other methods of forming prediction equations. With univariate PLS, linear combinations of the explanatory variables are formed sequentially and related to Y by ordinary least squares regression. It is shown that these linear combinations, here called components, may be viewed as weighted averages of predictors, where each predictor holds the residual information in an explanatory variable that is not contained in earlier components, and the quantity to be predicted is the vector of residuals from regressing Y against earlier components. A similar strategy is shown to underlie multivariate PLS, except that the quantity to be predicted is a weighted average of the residuals from separately regressing each Y variable against earlier components. This clarifies the differences between univariate and multivariate PLS, and it is argued that in most situations, the univariate method is likely to give the better prediction equations. In the example using simulation, univariate PLS is compared with four other methods of forming prediction equations: ordinary least squares, forward variable selection, principal components regression, and a Stein shrinkage method. Results suggest that PLS is a useful method for forming prediction equations when there are a large number of explanatory variables, particularly when the random error variance is large.},
  keywords = {Biased regression,Data reduction,Prediction,Regressor construction},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1994.10476452},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Garthwaite_1994_An_Interpretation_of_Partial_Least_Squares.pdf;/home/andrea/Zotero/storage/L66P8R2X/01621459.1994.html}
}

@book{ghosalFundamentalsNonparametricBayesian2017,
  title = {Fundamentals of {{Nonparametric Bayesian Inference}}},
  author = {Ghosal, Subhashis and van der Vaart, Aad},
  year = {2017},
  month = jun,
  publisher = {{Cambridge University Press}},
  abstract = {Explosive growth in computing power has made Bayesian methods for infinite-dimensional models - Bayesian nonparametrics - a nearly universal framework for inference, finding practical use in numerous subject areas. Written by leading researchers, this authoritative text draws on theoretical advances of the past twenty years to synthesize all aspects of Bayesian nonparametrics, from prior construction to computation and large sample behavior of posteriors. Because understanding the behavior of posteriors is critical to selecting priors that work, the large sample theory is developed systematically, illustrated by various examples of model and prior combinations. Precise sufficient conditions are given, with complete proofs, that ensure desirable posterior properties and behavior. Each chapter ends with historical notes and numerous exercises to deepen and consolidate the reader's understanding, making the book valuable for both graduate students and researchers in statistics and machine learning, as well as in application areas such as econometrics and biostatistics.},
  googlebooks = {cs8oDwAAQBAJ},
  isbn = {978-0-521-87826-5},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Ghosal_Vaart_2017_Fundamentals_of_Nonparametric_Bayesian_Inference.pdf}
}

@article{goldbergWhatEveryComputer1991,
  title = {What Every Computer Scientist Should Know about Floating-Point Arithmetic},
  author = {Goldberg, David},
  year = {1991},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {23},
  number = {1},
  pages = {5--48},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/103162.103163},
  url = {https://dl.acm.org/doi/10.1145/103162.103163},
  urldate = {2021-12-20},
  abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Goldberg_1991_What_every_computer_scientist_should_know_about_floating-point_arithmetic.pdf}
}

@article{hendersonVecVechOperators1979,
  title = {Vec and Vech Operators for Matrices, with Some Uses in Jacobians and Multivariate Statistics},
  author = {Henderson, Harold V. and Searle, S. R.},
  year = {1979},
  journal = {Canadian Journal of Statistics},
  volume = {7},
  number = {1},
  pages = {65--81},
  issn = {03195724, 1708945X},
  doi = {10.2307/3315017},
  url = {http://doi.wiley.com/10.2307/3315017},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Henderson_Searle_1979_Vec_and_vech_operators_for_matrices,_with_some_uses_in_jacobians_and.pdf}
}

@article{hurnEstimatingMixturesRegressions2003,
  title = {Estimating {{Mixtures}} of {{Regressions}}},
  author = {Hurn, Merrilee and Justel, Ana and Robert, Christian P},
  year = {2003},
  month = mar,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {12},
  number = {1},
  pages = {55--79},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/1061860031329},
  url = {https://doi.org/10.1198/1061860031329},
  urldate = {2022-01-11},
  abstract = {This article shows how Bayesian inference for switching regression models and their generalizations can be achieved by the specification of loss functions which overcome the label switching problem common to all mixture models. We also derive an extension to models where the number of components in the mixture is unknown, based on the birthand-death technique developed in recent literature. The methods are illustrated on various real datasets.},
  keywords = {Bayesian inference,Birth-and-death process,Label switching,Logistic regression,Loss functions,MCMC algorithms,Poisson regression,Switching regression},
  annotation = {\_eprint: https://doi.org/10.1198/1061860031329},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Hurn_et_al_2003_Estimating_Mixtures_of_Regressions.pdf;/home/andrea/Zotero/storage/W5SJM7TV/1061860031329.html}
}

@book{jacodProbabilityEssentials2003,
  title = {Probability Essentials},
  author = {Jacod, Jean and Protter, Philip E.},
  year = {2003},
  series = {Universitext},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-43871-7},
  lccn = {QA273 .J26 2003},
  keywords = {Probabilities},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Jacod_Protter_2003_Probability_essentials.pdf}
}

@article{khareBayesianApproachEnvelope2017,
  title = {A {{Bayesian}} Approach for Envelope Models},
  author = {Khare, Kshitij and Pal, Subhadip and Su, Zhihua},
  year = {2017},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {45},
  number = {1},
  pages = {196--222},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/16-AOS1449},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-1/A-Bayesian-approach-for-envelope-models/10.1214/16-AOS1449.full},
  urldate = {2021-09-16},
  abstract = {The envelope model is a new paradigm to address estimation and prediction in multivariate analysis. Using sufficient dimension reduction techniques, it has the potential to achieve substantial efficiency gains compared to standard models. This model was first introduced by [Statist. Sinica 20 (2010) 927\textendash 960] for multivariate linear regression, and has since been adapted to many other contexts. However, a Bayesian approach for analyzing envelope models has not yet been investigated in the literature. In this paper, we develop a comprehensive Bayesian framework for estimation and model selection in envelope models in the context of multivariate linear regression. Our framework has the following attractive features. First, we use the matrix Bingham distribution to construct a prior on the orthogonal basis matrix of the envelope subspace. This prior respects the manifold structure of the envelope model, and can directly incorporate prior information about the envelope subspace through the specification of hyperparamaters. This feature has potential applications in the broader Bayesian sufficient dimension reduction area. Second, sampling from the resulting posterior distribution can be achieved by using a block Gibbs sampler with standard associated conditionals. This in turn facilitates computationally efficient estimation and model selection. Third, unlike the current frequentist approach, our approach can accommodate situations where the sample size is smaller than the number of responses. Lastly, the Bayesian approach inherently offers comprehensive uncertainty characterization through the posterior distribution. We illustrate the utility of our approach on simulated and real datasets.},
  keywords = {60J20,62F15,62F30,62H12,envelope model,Gibbs sampling,matrix Bingham distribution,Stiefel manifold,sufficient dimension reduction},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Khare_et_al_2017_A_Bayesian_approach_for_envelope_models.pdf;/home/andrea/Zotero/storage/IPNEJJBR/16-AOS1449.html}
}

@article{luLikelihoodRatioTest2005,
  title = {The Likelihood Ratio Test for a Separable Covariance Matrix},
  author = {Lu, Nelson and Zimmerman, Dale L.},
  year = {2005},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {73},
  number = {4},
  pages = {449--457},
  issn = {01677152},
  doi = {10.1016/j.spl.2005.04.020},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167715205001495},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Lu_Zimmerman_2005_The_likelihood_ratio_test_for_a_separable_covariance_matrix.pdf}
}

@article{magnusCommutationMatrixProperties1979,
  ids = {magnusCommutationMatrixProperties1979a},
  title = {The {{Commutation Matrix}}: {{Some Properties}} and {{Applications}}},
  shorttitle = {The {{Commutation Matrix}}},
  author = {Magnus, Jan R. and Neudecker, H.},
  year = {1979},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {7},
  number = {2},
  issn = {0090-5364},
  doi = {10.1214/aos/1176344621},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-2/The-Commutation-Matrix-Some-Properties-and-Applications/10.1214/aos/1176344621.full},
  urldate = {2021-12-09},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Magnus_Neudecker_1979_The_Commutation_Matrix.pdf;/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Magnus_Neudecker_1979_The_Commutation_Matrix2.pdf}
}

@book{magnusMatrixDifferentialCalculus2019,
  title = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
  author = {Magnus, Jan R. and Neudecker, Heinz},
  year = {2019},
  series = {Wiley Series in Probability and Statistics},
  edition = {Third edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  isbn = {978-1-119-54119-6 978-1-119-54116-5},
  lccn = {QA188},
  keywords = {Differential calculus,Econometrics,Matrices,Statistics},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Magnus_Neudecker_2019_Matrix_differential_calculus_with_applications_in_statistics_and_econometrics.pdf}
}

@incollection{marinBayesianModellingInference2005,
  title = {Bayesian {{Modelling}} and {{Inference}} on {{Mixtures}} of {{Distributions}}},
  booktitle = {Handbook of {{Statistics}}},
  author = {Marin, Jean-Michel and Mengersen, Kerrie and Robert, Christian P.},
  editor = {Dey, D. K. and Rao, C. R.},
  year = {2005},
  month = jan,
  series = {Bayesian {{Thinking}}},
  volume = {25},
  pages = {459--507},
  publisher = {{Elsevier}},
  doi = {10.1016/S0169-7161(05)25016-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0169716105250162},
  urldate = {2022-01-05},
  abstract = {Mixture distributions comprise a finite or infinite number of components, possibly of different distributional types, that can describe different features of data. The Bayesian paradigm allows for probability statements to be made directly about the unknown parameters, prior or expert opinion to be included in the analysis, and hierarchical descriptions of both local-scale and global features of the model. This chapter aims to introduce the prior modeling, estimation, and evaluation of mixture distributions in a Bayesian paradigm. The chapter shows that mixture distributions provide a flexible, parametric framework for statistical modeling and analysis. Focus is on the methods rather than advanced examples, in the hope that an understanding of the practical aspects of such modeling can be carried into many disciplines. It also points out the fundamental difficulty in doing inference with such objects, along with a discussion about prior modeling, which is more restrictive than usual, and the constructions of estimators, which also is more involved than the standard posterior mean solution. Finally, this chapter gives some pointers to the related models and problems like mixtures of regressions and hidden Markov models as well as Dirichlet priors.},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Marin_et_al_2005_Bayesian_Modelling_and_Inference_on_Mixtures_of_Distributions.pdf;/home/andrea/Zotero/storage/JQY2ZYRN/S0169716105250162.html}
}

@book{martinBayesianModelingComputation2022,
  title = {Bayesian Modeling and Computation in {{Python}}},
  author = {Martin, Osvaldo},
  year = {2022},
  series = {Texts in Statistical Science},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  abstract = {"Bayesian Modeling and Computation in Python aims to help beginner Bayesian practitioners to become intermediate modelers. It uses a hands on approach with PyMC3, Tensorflow Probability, ArviZ and other libraries focusing on the practice of applied statistics with references to the underlying mathematical theory. The book starts with a refresher of the Bayesian Inference concepts. The second chapter introduces modern methods for Exploratory Analysis of Bayesian Models. With an understanding of these two fundamentals the subsequent chapters talk through various models including linear regressions, splines, time series, Bayesian additive regression trees. The final chapters include Approximate Bayesian Computation, end to end case studies showing how to apply Bayesian modelling in different settings, and a chapter about the internals of probabilistic programming languages. Finally the last chapter serves as a reference for the rest of the book by getting closer into mathematical aspects or by extending the discussion of certain topics. This book is written by contributors of PyMC3, ArviZ, Bambi, and Tensorflow Probability among other libraries"--},
  isbn = {978-0-367-89436-8 978-1-03-218029-8},
  lccn = {QA279.5 .M365 2022},
  keywords = {Bayesian statistical decision theory,Mathematical statistics,Python (Computer program language)},
  file = {/home/andrea/gdrive/phd/zotero_files/Martin_2022_Bayesian_modeling_and_computation_in_Python.pdf}
}

@article{millerMixtureModelsPrior2018,
  title = {Mixture {{Models With}} a {{Prior}} on the {{Number}} of {{Components}}},
  author = {Miller, Jeffrey W. and Harrison, Matthew T.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {340--356},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1255636},
  url = {https://doi.org/10.1080/01621459.2016.1255636},
  urldate = {2022-01-05},
  abstract = {A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of components\textemdash that is, to use a mixture of finite mixtures (MFM). The most commonly used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMs\textemdash an exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representation\textemdash and crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes. Supplementary materials for this article are available online.},
  pmid = {29983475},
  keywords = {Bayesian,Clustering,Density estimation,Model selection,Nonparametric},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2016.1255636},
  file = {/home/andrea/gdrive/phd/zotero_files/Miller_Harrison_2018_Mixture_Models_With_a_Prior_on_the_Number_of_Components.pdf;/home/andrea/Zotero/storage/BVT7KYR5/01621459.2016.html}
}

@article{robertBayesianEstimationHidden1993,
  title = {Bayesian Estimation of Hidden {{Markov}} Chains: A Stochastic Implementation},
  shorttitle = {Bayesian Estimation of Hidden {{Markov}} Chains},
  author = {Robert, Christian P. and Celeux, Gilles and Diebolt, Jean},
  year = {1993},
  month = jan,
  journal = {Statistics \& Probability Letters},
  volume = {16},
  number = {1},
  pages = {77--83},
  issn = {01677152},
  doi = {10.1016/0167-7152(93)90127-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0167715293901275},
  urldate = {2022-01-13},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Robert_et_al_1993_Bayesian_estimation_of_hidden_Markov_chains.pdf}
}

@article{robertSimulationTruncatedNormal1995,
  title = {Simulation of Truncated Normal Variables},
  author = {Robert, Christian P.},
  year = {1995},
  month = jun,
  journal = {Statistics and Computing},
  volume = {5},
  number = {2},
  eprint = {0907.4010},
  eprinttype = {arxiv},
  pages = {121--125},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/BF00143942},
  url = {http://arxiv.org/abs/0907.4010},
  urldate = {2022-01-26},
  abstract = {We provide in this paper simulation algorithms for one-sided and two-sided truncated normal distributions. These algorithms are then used to simulate multivariate normal variables with restricted parameter space for any covariance structure.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Robert_1995_Simulation_of_truncated_normal_variables.pdf;/home/andrea/Zotero/storage/4889JST3/0907.html}
}

@article{royImplementationTestKronecker2005,
  title = {On Implementation of a Test for {{Kronecker}} Product Covariance Structure for Multivariate Repeated Measures Data},
  author = {Roy, Anuradha and Khattree, Ravindra},
  year = {2005},
  month = dec,
  journal = {Statistical Methodology},
  volume = {2},
  number = {4},
  pages = {297--306},
  issn = {15723127},
  doi = {10.1016/j.stamet.2005.07.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1572312705000389},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Roy_Khattree_2005_On_implementation_of_a_test_for_Kronecker_product_covariance_structure_for.pdf}
}

@article{shitanAsymptoticTestSeparability1995,
  title = {An Asymptotic Test for Separability of a Spatial Autoregressive Model},
  author = {Shitan, Mahendran and Brockwell, Peter J.},
  year = {1995},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {24},
  number = {8},
  pages = {2027--2040},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610929508831600},
  url = {http://www.tandfonline.com/doi/abs/10.1080/03610929508831600},
  urldate = {2021-12-09},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Shitan_Brockwell_1995_An_asymptotic_test_for_separability_of_a_spatial_autoregressive_model.pdf}
}

@article{stonekingBayesianInferenceGaussian2014,
  title = {Bayesian Inference of {{Gaussian}} Mixture Models with Noninformative Priors},
  author = {Stoneking, Colin J.},
  year = {2014},
  month = may,
  url = {https://arxiv.org/abs/1405.4895v1},
  urldate = {2022-01-11},
  abstract = {This paper deals with Bayesian inference of a mixture of Gaussian distributions. A novel formulation of the mixture model is introduced, which includes the prior constraint that each Gaussian component is always assigned a minimal number of data points. This enables noninformative improper priors such as the Jeffreys prior to be placed on the component parameters. We demonstrate difficulties involved in specifying a prior for the standard Gaussian mixture model, and show how the new model can be used to overcome these. MCMC methods are given for efficient sampling from the posterior of this model.},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Stoneking_2014_Bayesian_inference_of_Gaussian_mixture_models_with_noninformative_priors.pdf;/home/andrea/Zotero/storage/N6EJK3L8/1405.html}
}

@article{vieleModelingMixturesLinear2002,
  title = {Modeling with {{Mixtures}} of {{Linear Regressions}}},
  author = {Viele, Kert and Tong, Barbara},
  year = {2002},
  month = oct,
  journal = {Statistics and Computing},
  volume = {12},
  number = {4},
  pages = {315--330},
  issn = {1573-1375},
  doi = {10.1023/A:1020779827503},
  url = {https://doi.org/10.1023/A:1020779827503},
  urldate = {2022-01-11},
  abstract = {Consider data (x1,y1),...,(xn,yn), where each xi may be vector valued, and the distribution of yi given xi is a mixture of linear regressions. This provides a generalization of mixture models which do not include covariates in the mixture formulation. This mixture of linear regressions formulation has appeared in the computer science literature under the name ``Hierarchical Mixtures of Experts'' model.},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Viele_Tong_2002_Modeling_with_Mixtures_of_Linear_Regressions.pdf}
}

@article{zhuBayesianGeneralizedLow2014,
  title = {Bayesian {{Generalized Low Rank Regression Models}} for {{Neuroimaging Phenotypes}} and {{Genetic Markers}}},
  author = {Zhu, Hongtu and Khondker, Zakaria and Lu, Zhaohua and Ibrahim, Joseph G.},
  year = {2014},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {109},
  number = {507},
  pages = {977--990},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2014.923775},
  url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2014.923775},
  urldate = {2022-02-22},
  langid = {english},
  file = {/home/andrea/Insync/andrea.mascaretti@studenti.unipd.it/Google Drive/phd/zotero_files/Zhu_et_al_2014_Bayesian_Generalized_Low_Rank_Regression_Models_for_Neuroimaging_Phenotypes_and.pdf}
}


